{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Wuzzuf Job Website\n",
    "\n",
    "This code scrapes the Wuzzuf job website which has about ~5K jobs in Egypt for the assignment \"Improving Labor Market Matching in Egypt.\"\n",
    "\n",
    "The Wuzzuf job website has detailed stats on the number of job applicants, reviewed applicants by employer, and number of rejected applicants.  It also is all in English making it easier to synthesize the data.  Plus there is plenty of tagging for job category as well as industry codes.  The data is subsequently output into a csv file that is further processed and cleaned.\n",
    "\n",
    "#### Created by Natalie Chun (20 September - 15 October 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#key packages for scraping websites\n",
    "import urllib\n",
    "import urllib.request as urlrequest\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "\n",
    "#standard packages for holding and analyzing datasets and outputting files\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get Egypt Jobs (only)\n",
    "def get_WuzuffJobUrls():\n",
    "\n",
    "    url = 'https://wuzzuf.net/search/jobs?start=0&filters%5Bcountry%5D%5B0%5D=Egypt'\n",
    "    nextpage = True\n",
    "    url_jobs = []\n",
    "\n",
    "    while nextpage:\n",
    "    \n",
    "        req = urlrequest.Request(url)\n",
    "        response = urlrequest.urlopen(req)\n",
    "        soup = BeautifulSoup(response, 'html.parser')\n",
    "    \n",
    "        # objective is to get the links from the page and put it in a list to call and run through\n",
    "        name_box = soup.find('div', attrs={'class': 'content-card card-has-jobs'})\n",
    "        #print(name_box)\n",
    "\n",
    "        #obtain all of the urls associated with different jobs listed on the website (this only needs to be called once)\n",
    "        for a in name_box.find_all('a', href=True):\n",
    "            if 'https://wuzzuf.net/jobs/p/' in a['href']:\n",
    "                href = a['href'].split('?')\n",
    "                if href[0] not in url_jobs:\n",
    "                    url_jobs.append(href[0])\n",
    "                    #print(\"Found the URL:\", href[0])\n",
    "\n",
    "        # get the next set of job listings for this classification\n",
    "        nextpg = name_box.find('li', attrs={'class': 'pag-next'})\n",
    "        try:\n",
    "            url = nextpg.find_all('a', href=True)[0]['href']\n",
    "            #Print out length to track number of urls retrieved\n",
    "            #print(len(url_jobs))\n",
    "            # sleep so that we do not bombard the website with requests\n",
    "            time.sleep(5)\n",
    "        except AttributeError:\n",
    "            nextpage = False\n",
    "    \n",
    "    print(len(url_jobs))\n",
    "    return(url_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write the job urls out to a file so we do not have to continually call this if we do not want to\n",
    "job_urls = get_WuzuffJobUrls()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#write the urllinks to be stored in a file\n",
    "datenow = time.strftime(\"%m%d%Y\")\n",
    "with open('Wuzzuf_job_urls'+datenow+'.txt', 'w', newline='') as file:\n",
    "    for j in job_urls:\n",
    "        file.write(str(j.encode('utf-8')))\n",
    "        file.write(\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://wuzzuf.net/jobs/p/102624-Sales-Agent---Multinational-Insurance-Company-Allianz-Egypt-Cairo-Egypt', 'https://wuzzuf.net/jobs/p/102623-Offshore-Account----Call-Center-Representative---Business-International-Services-Cairo-Egypt', 'https://wuzzuf.net/jobs/p/102622-Marketing-Manager-Elshawa-Trading-Group-Dakahlia-Egypt', 'https://wuzzuf.net/jobs/p/100707-Senior-English-Instructor-Harvest-British-College-Alexandria-Egypt', 'https://wuzzuf.net/jobs/p/102616-photographer-Ben-Soliman-Giza-Egypt']\n"
     ]
    }
   ],
   "source": [
    "datenow = time.strftime(\"%m%d%Y\")\n",
    "job_urls = []\n",
    "with open('Wuzzuf_job_urls'+datenow+'.txt', 'r') as file:\n",
    "    for l in file:\n",
    "        job_urls.append(l.strip('''b\\'''').strip('''\\'\\n'''))\n",
    "print(job_urls[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this function takes the job urls and calls another function to scrape each job advertisement page\n",
    "scrape_starttime = datetime.datetime.now()\n",
    "\n",
    "def scrapeWuzzufPages(urls,cnt,replace):\n",
    "    sys.stdout.encoding\n",
    "    if replace is True:\n",
    "        filewrite = 'w'\n",
    "    else:\n",
    "        filewrite = 'a+'\n",
    "        \n",
    "    with open('Wuzzuf_jobdata_'+datenow+'.csv', filewrite, newline='') as file:\n",
    "        w = csv.writer(file)\n",
    "        if replace is True:\n",
    "            w.writerow([\"download_date\",\"job-title\", 'job-company-name', \"job-company-location\", \"postdate\", \"num_applicants\",\"num_vacancies\", \"num_reviewed\",\n",
    "               \"num_shortlist\", \"num_rejected\", 'experience_needed', 'career_level', 'job_type', 'salary',\n",
    "                        'education_level','gender','travel_frequency','languages','vacancies','job_roles','keywords','requirements','industries'])\n",
    "\n",
    "        i = cnt\n",
    "        for url in urls:\n",
    "            try:\n",
    "                w.writerow(get_WuzzufJobData(url))\n",
    "            except:\n",
    "                pass\n",
    "            #sleep for a bit too make sure to not hit pages too often\n",
    "            time.sleep(3)\n",
    "            i += 1\n",
    "            if i % 100 == 0:\n",
    "                print('Processed urls: {}'.format(i))\n",
    "                \n",
    "        print(\"\\nDone!\\n Statuses Processed in %s\" \n",
    "              % (datetime.datetime.now() - scrape_starttime))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "punctuation = [\";\",\",\",\"'\",\"&\"]\n",
    "\n",
    "#Function keeps requesting page or until it hits limit of 5 requests\n",
    "def request_until_succeed(url):\n",
    "    req = urlrequest.Request(url)\n",
    "    success = False\n",
    "    while success is False:\n",
    "        try: \n",
    "            response = urlrequest.urlopen(req)\n",
    "            if response.getcode() == 200:\n",
    "                success = True\n",
    "        except Exception:\n",
    "            try:\n",
    "                urlsplit = urllib.parse.urlsplit(url)\n",
    "                urlsplit = list(urlsplit)\n",
    "                print(urlsplit)\n",
    "                urlsplit[2] = urllib.parse.quote(urlsplit[2])\n",
    "                url = urllib.parse.urlunsplit(urlsplit)\n",
    "                req = urlrequest.Request(url)\n",
    "            except Exception:\n",
    "                print(\"Exception\")\n",
    "                time.sleep(5)\n",
    "                print(\"Error for URL %s: %s\" % (url, datetime.datetime.now()))\n",
    "\n",
    "    return response.read()\n",
    "\n",
    "#this page scrapes individual job advertisement pages\n",
    "def get_WuzzufJobData(urlname):\n",
    "\n",
    "    response = request_until_succeed(urlname)\n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    #print(soup)\n",
    "    \n",
    "    #obtain main job data\n",
    "    mainjobdata = soup.find('div', attrs={'class': 'job-main-card content-card'})\n",
    "    #print(mainjobdata)\n",
    "    jobdata = mainjobdata.find_all(['h1','a','span'])\n",
    "    #print(jobdata)\n",
    "    jobinfo = {}\n",
    "    for d in jobdata:\n",
    "        try:\n",
    "            if d['class'][0] in ['job-title','job-company-name','job-company-location']:\n",
    "                jobinfo[d['class'][0]] = d.get_text().strip().encode('utf-8')\n",
    "                #print(jobinfo[d['class'][0]])\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    #get stats on applicants\n",
    "    try:\n",
    "        num_applicants = mainjobdata.find_all('div', attrs={'class': 'applicants-num'})[0].get_text()\n",
    "    except IndexError:\n",
    "        num_applicants = 0\n",
    "        \n",
    "    try:\n",
    "        num_vacancies = mainjobdata.find_all('span', attrs={'class': 'vacancies-num'})[0].get_text()\n",
    "    except IndexError:\n",
    "        num_vacancies = 0\n",
    "\n",
    "    stats = mainjobdata.find_all('div', attrs={'class': 'applicants-stat-num'})\n",
    "    #print(stats)\n",
    "    try:\n",
    "        num_seen = stats[0].get_text()\n",
    "    except IndexError:\n",
    "        num_seen = 0\n",
    "    try:\n",
    "        num_shortlist = stats[1].get_text()\n",
    "    except IndexError:\n",
    "        num_shortlist = 0\n",
    "    try:\n",
    "        num_rejected = stats[2].get_text()\n",
    "    except IndexError:\n",
    "        num_rejected = 0\n",
    "        \n",
    "    #get date when posted and download date\n",
    "    post_date = mainjobdata.find('p', attrs={'class': 'job-post-date'})\n",
    "    #print(post_date['title'])\n",
    "    try:\n",
    "        dateval = datetime.datetime.strptime(post_date['title'],'%A, %B %d, %Y at %H:%M%p')\n",
    "    except ValueError:\n",
    "        dateval = datetime.datetime.strptime(post_date['title'],'%A, %B %d, %Y at%I:%M%p')\n",
    "    dateval = dateval.strftime('%Y-%m-%d %H:%M') # best time format for spreadsheet programs\n",
    "    \n",
    "    #now still need to split the post-date into a term that is valid\n",
    "    #print(post_date['title'])\n",
    "    \n",
    "    #obtain job summary information\n",
    "    jobsumm = soup.find('div', attrs={'class': 'row job-summary'})\n",
    "    jobsummdata = jobsumm.find_all(['dl'])\n",
    "    #print(jobsumm)\n",
    "    #print(jobdata)\n",
    "    for d in jobsummdata:\n",
    "        try:\n",
    "            temp = re.sub('\\s+',' ',d.get_text()).strip().split(\":\")\n",
    "            name = re.sub('\\s',\"_\",temp[0].lower())\n",
    "            if name in ['languages']:\n",
    "                jobinfo[name] = temp[1].strip().split(',')\n",
    "            elif name in ['salary']:\n",
    "                if 'Negotiable' in temp[1].strip().split(','):\n",
    "                    jobinfo[name] = temp[1].strip().split(',')\n",
    "                else:\n",
    "                    newtemp = temp[1].strip().replace(',','')\n",
    "                    jobinfo[name] = [newtemp]\n",
    "            else:\n",
    "                jobinfo[name] = temp[1].strip()\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    #these columns are not consistent across jobs so need to take this into account\n",
    "    columns = ['experience_needed','career_level','job_type','salary','education_level','gender','travel_frequency','languages','vacancies']\n",
    "    for c in columns:\n",
    "        if c not in jobinfo:\n",
    "            jobinfo[c] = \"NA\"\n",
    "       \n",
    "    jobcard = soup.find('div', attrs={'class': \"about-job content-card\"})\n",
    "    #print(jobcard)\n",
    "    data = jobcard.find_all('div', attrs={'class': \"labels-wrapper\"})\n",
    "    #print(data)\n",
    "    jobroles = []\n",
    "    for d in data:\n",
    "        for role in d.find_all(['a']):\n",
    "            jobroles.append(role.get_text().strip())    \n",
    "    jobinfo['roles'] = jobroles\n",
    "    #print(jobroles)\n",
    "        \n",
    "    #obtain job requirements, key words, and industry indicators\n",
    "    jobreqs = soup.find('div', attrs={'class': \"job-requirements content-card\"})\n",
    "    #print(jobreqs)\n",
    "    if jobreqs is not None:\n",
    "        data = jobreqs.find_all('meta', content=True)\n",
    "        keywords = []\n",
    "        try:\n",
    "            temp = data[0]['content'].replace('◌ِ','')\n",
    "            for t in temp.split(', '):\n",
    "                keywords.append(t.encode('utf-8'))\n",
    "            jobinfo['keywords'] = keywords\n",
    "        except IndexError:\n",
    "            jobinfo['keywords'] = []\n",
    "    else:\n",
    "        jobinfo['keywords'] = []\n",
    "    #print(jobinfo['keywords'])\n",
    "    \n",
    "    try:\n",
    "        data = jobreqs.find_all('li')\n",
    "        reqs = []\n",
    "        for d in data:\n",
    "            temp = d.get_text().lower().strip('.')\n",
    "            for p in punctuation:\n",
    "                temp = temp.replace(';','')\n",
    "            reqs.append(temp.encode('utf-8'))\n",
    "        jobinfo['requirements'] = reqs\n",
    "        #print(reqs)\n",
    "    except:\n",
    "        jobinfo['requirements'] = []\n",
    "    \n",
    "    industries = soup.find('div', attrs={'class': \"industries labels-wrapper\"})\n",
    "    #print(industries.find_all(['a']))\n",
    "    inds = []\n",
    "    for ind in industries.find_all(['a']):\n",
    "        inds.append(ind.get_text().strip().encode('utf-8'))\n",
    "    #print(inds)\n",
    "    jobinfo['industries'] = inds\n",
    "    \n",
    "    #print(jobinfo)\n",
    "    # now let us return the dictionary entries to write to a csv file.  \n",
    "    #Note that we may need to split so we do not have problem with commas\n",
    "    job_data = [datenow,jobinfo['job-title'],jobinfo['job-company-name'],jobinfo['job-company-location'],dateval,num_applicants,num_vacancies,num_seen,\n",
    "               num_shortlist,num_rejected,jobinfo['experience_needed'],jobinfo['career_level'],jobinfo['job_type'],jobinfo['salary'],\n",
    "               jobinfo['education_level'],jobinfo['gender'],jobinfo['travel_frequency'],jobinfo['languages'],jobinfo['vacancies'],jobinfo['roles'],jobinfo['keywords'],jobinfo['requirements'],jobinfo['industries']]\n",
    "    \n",
    "    return(job_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wuzzuf.net/jobs/p/101927-Social-Media-Secretary-Auto-Reda-Hamza-Cairo-Egypt\n",
      "['11292017', b'Social Media Secretary', b'Auto Reda Hamza', b'Heliopolis, Cairo', '2017-11-25 16:22', '30', '1', '10', '3', 0, 'More than 3 years', 'Experienced (Non-Manager)', 'Full Time', ['Negotiable', ' مرتبات وحوافز'], 'NA', 'NA', 'NA', 'NA', '1 open position', ['Administration', 'Media/Journalism/Publishing'], [b'Admin', b'Administration', b'Secretary', b'Social Media', b'Media'], [], [b'Automotive']]\n"
     ]
    }
   ],
   "source": [
    "# error test of the data\n",
    "print(job_urls[640])\n",
    "print(get_WuzzufJobData(job_urls[640]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#scrape the data.  either start from beginning or just append to existing file\n",
    "replace = False\n",
    "\n",
    "if replace == True:\n",
    "    scrapeWuzzufPages(job_urls,0,True)\n",
    "else:\n",
    "    #Test trial for data entry into csv file (read-in)\n",
    "    data = pd.read_csv('Wuzzuf_jobdata_'+datenow+'.csv',encoding='utf-8',usecols=['job-title'])\n",
    "    print(len(data))\n",
    "    print(job_urls[len(data)])\n",
    "    scrapeWuzzufPages(job_urls[len(data):],len(data),False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
