{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape OLX Website\n",
    "\n",
    "This was created for the paper \"Improving Labor Market Matching in Egypt\".  It does an initial scrape of the OLX websites which are seemingly one of the most comprehensive sites for online job postings that are available in Egypt.  This code scrapes the following OLX data:\n",
    "1. Aggregate counts by region of ad postings\n",
    "2. Aggregate counts by region of job ad postings\n",
    "3. Counts by region and sector of job ad postings\n",
    "\n",
    "The data is subsequently output into a csv file that can be read in for analysis.\n",
    "\n",
    "Note:  In the future, consideration may be given to scraping the individual job ads which contain more detailed information on a) desired level of education b) job data c) experience d) position type.  However, in the initial assessment the difficulty of translating from Arabic into English and scraping the individual job ads was seen as too time consuming within the time that was available and allocated for this assignment.  Job ads seemingly are quite messy and a significant amount of time would be needed to clean the fields and obtain consistency.\n",
    "\n",
    "The following package may be potentially useful for automatic translation (google)\n",
    "https://gist.github.com/jseabold/1473363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request as urlrequest\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "import csv\n",
    "datenow = time.strftime(\"%m%d%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this URL has the regions in which it is possible to grab all the locational based data for the jobs\n",
    "def get_OLXregiondata():\n",
    "\n",
    "\n",
    "    url = 'https://olx.com.eg/en/sitemap/regions/'\n",
    "    req = urlrequest.Request(url)\n",
    "    response = urlrequest.urlopen(req)\n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    #print(soup)\n",
    "    \n",
    "    name_box = soup.find('div', attrs={'class': 'content text'})\n",
    "    regions = name_box.find_all('div', attrs={'class':'bgef pding5_10 marginbott10 margintop20 clr'})\n",
    "    subregions = name_box.find_all('div',attrs={'class':\"clr marginbott10\"})\n",
    "    #print(len(regions))\n",
    "    #print(len(subregions))\n",
    "\n",
    "    regionname = []\n",
    "    totalposts = []\n",
    "    subregname = []\n",
    "    subposts = []\n",
    "    fregname = []\n",
    "    fsubregname = []\n",
    "    data = []\n",
    "    \n",
    "    for i, subreg in enumerate(subregions):\n",
    "    #print(regions[i].get_text().strip())\n",
    "        region = regions[i].get_text().strip()\n",
    "        temp = region.split(' (')\n",
    "        regionname.append(temp[0])\n",
    "        totalposts.append(temp[1].strip(')')) \n",
    "        fregname.append(re.sub('\\s+(\\+\\s)?','-',regionname[i].lower()))    \n",
    "        #print(subreg)\n",
    "        text = subreg.find_all('li')\n",
    "        #print(text)\n",
    "        for t in text:\n",
    "            temp = t.get_text().strip()\n",
    "            temp = temp.replace('\\n','').split('(')\n",
    "            #print(temp)\n",
    "            subregname = temp[0].strip()\n",
    "            subposts = temp[1].strip(')')\n",
    "            #print(subregname,subposts)\n",
    "            fsubregname = re.sub('''[\\s(\\+\\s)?|\\'|\\.\\s]''','-',subregname.lower())\n",
    "            fsubregname = re.sub('[-](-)?(-)?','-',fsubregname)\n",
    "            data.append([datenow,regionname[i],fregname[i],totalposts[i],subregname,fsubregname,subposts])\n",
    "            #print(\"Region {}, Total Posts {}, Subregion {}, Subposts {}\".format(regionname,totalposts,subregname,subposts))\n",
    "            #want to export data to different csv\n",
    "      \n",
    "    #write out the data for 359 districts\n",
    "    with open('OLX_regiondata_'+datenow+'.csv', 'w', newline='') as file:\n",
    "        w = csv.writer(file)\n",
    "        w.writerow([\"download_date\",\"region\",'fregname','totalposts',\"subregion\",'fsubregname',\"subposts\"])\n",
    "        for d in data:\n",
    "            w.writerow(d) \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_OLXregiondata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   download_date region fregname  totalposts      subregion    fsubregname  \\\n",
      "0       11302017  Aswan    aswan        3278    Abou Simbel    abou-simbel   \n",
      "1       11302017  Aswan    aswan        3278  Abou al-Reish  abou-al-reish   \n",
      "2       11302017  Aswan    aswan        3278     Aswan City     aswan-city   \n",
      "3       11302017  Aswan    aswan        3278       Basiliah       basiliah   \n",
      "4       11302017  Aswan    aswan        3278          Daraw          daraw   \n",
      "\n",
      "   subposts  \n",
      "0        24  \n",
      "1        54  \n",
      "2      2311  \n",
      "3         9  \n",
      "4        72  \n"
     ]
    }
   ],
   "source": [
    "#now our objective is to get the sectoral job data for each region-area (one million job postings)\n",
    "dateval = '11302017'\n",
    "data = pd.read_csv('OLX_regiondata_'+dateval+'.csv')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_OLXJobUrls(url):\n",
    "    \n",
    "    sector = {}\n",
    "    href = {}\n",
    "    \n",
    "    req = urlrequest.Request(url)\n",
    "    try:\n",
    "        response = urlrequest.urlopen(req)\n",
    "    #certain regions have no job postings\n",
    "    except:\n",
    "        return([sector,href])\n",
    "        \n",
    "        \n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "\n",
    "    #get counts of number of jobs in different areas\n",
    "    name_box = soup.find_all('div', attrs={'class': 'wrapper'})\n",
    "    \n",
    "    for name in name_box:\n",
    "        #print(name)\n",
    "        newnames = name.find_all('a', attrs={'class' : 'topLink tdnone '})\n",
    "        if len(newnames) > 0:\n",
    "            for i, n in enumerate(newnames):\n",
    "                #print(n)\n",
    "                #print(n['href'])\n",
    "                #print(n.find(href=True))\n",
    "                #href.append(n.find('a', href=True))\n",
    "                sect = n.find('span', attrs='link').get_text().strip()\n",
    "                cnt = n.find('span', attrs='counter nowrap').get_text().strip().replace(',','')\n",
    "                #export a tuple rather than dictionary\n",
    "                sector[sect] = cnt\n",
    "                href[sect] = n['href']\n",
    "    #print(sector)\n",
    "    #print(href)\n",
    "    return([sector,href])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets first obtain the full country data to pull-in data on the different job categories\n",
    "sector, href = get_OLXJobUrls('https://olx.com.eg/en/jobs-services/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://olx.com.eg/en/jobs-services/ramses-ramses-extension/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Accounting': '7',\n",
       "  'Architecture - Engineering': '1',\n",
       "  'Construction': '2',\n",
       "  'Consulting': '2',\n",
       "  'Education': '3',\n",
       "  'Executive': '1',\n",
       "  'Hospitality': '1',\n",
       "  'IT - Telecom': '1',\n",
       "  'Jobs Wanted': '30',\n",
       "  'Marketing - PR': '8',\n",
       "  'Medical - Health': '3',\n",
       "  'Other': '43',\n",
       "  'Retail': '8',\n",
       "  'Sales': '7',\n",
       "  'Secretarial': '2'},\n",
       " {'Accounting': 'https://olx.com.eg/en/jobs-services/accounting/ramses-ramses-extension/',\n",
       "  'Architecture - Engineering': 'https://olx.com.eg/en/jobs-services/architectureengineering/ramses-ramses-extension/',\n",
       "  'Construction': 'https://olx.com.eg/en/jobs-services/construction/ramses-ramses-extension/',\n",
       "  'Consulting': 'https://olx.com.eg/en/jobs-services/consulting/ramses-ramses-extension/',\n",
       "  'Education': 'https://olx.com.eg/en/jobs-services/education/ramses-ramses-extension/',\n",
       "  'Executive': 'https://olx.com.eg/en/jobs-services/executive/ramses-ramses-extension/',\n",
       "  'Hospitality': 'https://olx.com.eg/en/jobs-services/hospitality/ramses-ramses-extension/',\n",
       "  'IT - Telecom': 'https://olx.com.eg/en/jobs-services/ittelecom/ramses-ramses-extension/',\n",
       "  'Jobs Wanted': 'https://olx.com.eg/en/jobs-services/jobs-wanted/ramses-ramses-extension/',\n",
       "  'Marketing - PR': 'https://olx.com.eg/en/jobs-services/marketingpr/ramses-ramses-extension/',\n",
       "  'Medical - Health': 'https://olx.com.eg/en/jobs-services/medicalhealth/ramses-ramses-extension/',\n",
       "  'Other': 'https://olx.com.eg/en/jobs-services/jobs-other/ramses-ramses-extension/',\n",
       "  'Retail': 'https://olx.com.eg/en/jobs-services/retail/ramses-ramses-extension/',\n",
       "  'Sales': 'https://olx.com.eg/en/jobs-services/sales/ramses-ramses-extension/',\n",
       "  'Secretarial': 'https://olx.com.eg/en/jobs-services/secretarial/ramses-ramses-extension/'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://olx.com.eg/en/jobs-services/'+'ramses-ramses-extension'+'/'\n",
    "print(url)\n",
    "get_OLXJobUrls(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now we want to loop through the key industries above and regions to investigate the counts of postings\n",
    "# under each heading\n",
    "# an easier way is to just loop through the general regions  \n",
    "    \n",
    "def write_OLXregionjobdata():\n",
    "    \n",
    "    #sector names\n",
    "    sectornames = []\n",
    "    for key, val in sector.items():\n",
    "        sectornames.append(key)\n",
    "    \n",
    "    #write out the data for 359 districts\n",
    "    with open('OLX_regionjobdata_'+datenow+'.csv', 'w', newline='') as file:\n",
    "        w = csv.writer(file)\n",
    "        w.writerow([\"region\",\"subregname\"]+sectornames)\n",
    "\n",
    "        #loop through 365 qism areas to get job data\n",
    "        for i, reg in data.iterrows():\n",
    "            fsubregname = reg['fsubregname']\n",
    "            url = 'https://olx.com.eg/en/jobs-services/' + fsubregname + '/'\n",
    "            subregsector, subreghref = get_OLXJobUrls(url)\n",
    "            #now want to output this data into a csv file\n",
    "            cntdata = []\n",
    "            for key, val in sector.items():\n",
    "                if key in subregsector:\n",
    "                    cntdata.append(int(subregsector[key]))\n",
    "                else:\n",
    "                    cntdata.append(0)\n",
    "            rowdata = [reg['region'],reg['subregion']]+cntdata\n",
    "            if i % 20 == 0:\n",
    "                print(rowdata)\n",
    "            w.writerow(rowdata)\n",
    "            # sleep to make sure not too many requests are being made\n",
    "            time.sleep(1)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aswan', 'Abou Simbel', 0, 0, 0, 0, 7, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['Asyut', 'New Assiut', 3, 11, 27, 5, 84, 1, 14, 0, 0, 3, 0, 0, 1, 0, 0, 2, 0, 2]\n",
      "['Alexandria', 'Bolkly', 2, 0, 8, 4, 11, 4, 1, 2, 0, 0, 1, 0, 2, 0, 1, 0, 0, 5]\n",
      "['Alexandria', 'San Stefano', 2, 18, 5, 4, 24, 5, 9, 4, 0, 0, 2, 0, 0, 1, 1, 0, 0, 4]\n",
      "['Red Sea', 'Qusair', 0, 1, 1, 0, 2, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['Beheira', 'Wadi al-Natrun', 0, 0, 1, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0]\n",
      "['Giza', 'Kerdasa', 2, 0, 9, 1, 20, 1, 2, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1]\n",
      "['Giza', 'Warraq', 26, 49, 168, 14, 251, 13, 23, 9, 17, 6, 8, 1, 18, 2, 6, 2, 12, 9]\n",
      "['Suez', 'Arbaeen', 1, 2, 1, 1, 28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['Sharqia', 'Qareen', 0, 0, 1, 0, 25, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "['Cairo', 'Ain Shams', 78, 93, 300, 41, 670, 46, 62, 42, 28, 10, 29, 1, 34, 10, 9, 4, 19, 38]\n",
      "['Cairo', 'New Cairo', 119, 212, 277, 140, 1075, 66, 170, 122, 57, 117, 49, 8, 116, 16, 59, 20, 41, 47]\n",
      "['Qalyubia', 'Khanka', 4, 5, 24, 2, 80, 1, 2, 1, 2, 2, 1, 0, 3, 1, 4, 0, 1, 3]\n",
      "['Minya', 'Adwa', 0, 0, 9, 0, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['Beni Suef', 'Nasser', 0, 1, 3, 0, 2, 4, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "['Damietta', 'Damietta City', 1, 18, 87, 6, 54, 13, 23, 2, 3, 2, 4, 1, 5, 0, 5, 1, 3, 7]\n",
      "['Sohag', 'Sohag City', 8, 21, 88, 10, 131, 8, 29, 4, 4, 7, 7, 0, 13, 1, 2, 0, 9, 5]\n",
      "['Kafr al-Sheikh', 'Desouk', 0, 1, 11, 11, 20, 4, 3, 0, 1, 2, 1, 0, 0, 0, 2, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "write_OLXregionjobdata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape individual job advertisement pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#gets all the job page urls from subregion-sector listings\n",
    "def get_OLXJobPageUrls(url):\n",
    "    \n",
    "    urllist = []\n",
    "    req = urlrequest.Request(url)\n",
    "    response = urlrequest.urlopen(req)\n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    \n",
    "    #now find out the total number of pages available\n",
    "    try:\n",
    "        nextpage = soup.find('div',attrs={'class':'pager rel clr'})\n",
    "        temp = nextpage.find('input',attrs={'type':\"submit\"})['class']\n",
    "        totalpages = re.search(r'(\\d+)',str(temp[1])).group(1)\n",
    "        #print(temp,totalpages)\n",
    "    except:\n",
    "        totalpages = 1\n",
    "    \n",
    "    for i in range(1,int(totalpages)+1):\n",
    "        #print(\"Enter\")\n",
    "        newurl = url\n",
    "        if i > 1:\n",
    "            newurl = url + '/?page='+str(i) \n",
    "        req = urlrequest.Request(newurl)\n",
    "        response = urlrequest.urlopen(req)\n",
    "        soup = BeautifulSoup(response, 'html.parser')\n",
    "    \n",
    "        #<div class=\"ads__item \" onclick=\"window.location = 'https://olx.com.eg/ad/15-ID8nT4F.html'\">\n",
    "        adlinks = soup.find_all('div',attrs={'class':'ads__item__info'})\n",
    "        #print(adlinks)\n",
    " \n",
    "        #get all of the job ad page links\n",
    "        for ad in adlinks:\n",
    "            urllist.append(ad.find('a')['href'])\n",
    "        #print(len(urllist))\n",
    "        time.sleep(1)\n",
    "    return(urllist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create file listing of all job page urls (should be about ~100K)\n",
    "\n",
    "def write_OLXJobUrls(sector,datast=0):\n",
    "    #sector names\n",
    "    sectornames = []\n",
    "    for key, val in sector.items():\n",
    "        if key != 'Jobs Wanted':\n",
    "            sectornames.append(key)\n",
    "        \n",
    "    #loop through 365 qism areas to get job data\n",
    "    for i, reg in data[datast:].iterrows():\n",
    "            \n",
    "        print(reg['fsubregname'])\n",
    "        #write each regional data to a separate file to reduce having to re-do downloads in case of failure\n",
    "        with open('OLX_joburls_'+reg['fsubregname']+'_'+dateval+'.csv', 'w', newline='') as file:\n",
    "            w = csv.writer(file)\n",
    "            w.writerow([\"sector\",\"region\",\"subregion\",\"jobpageurl\"])\n",
    "            \n",
    "            fsubregname = reg['fsubregname']\n",
    "            url = 'https://olx.com.eg/en/jobs-services/' + fsubregname + '/'\n",
    "            subregsector, subreghref = get_OLXJobUrls(url)\n",
    "            \n",
    "            #now want to grab each of the urls for each subreg and subsector\n",
    "            for subregsector, href in subreghref.items():\n",
    "                #print(href)\n",
    "                urlpages = get_OLXJobPageUrls(href)\n",
    "                #print(len(urlpages))\n",
    "                for urlp in urlpages:\n",
    "                    w.writerow([subregsector,reg['region'],reg['subregion'],urlp])\n",
    "            if i % 20 == 0:\n",
    "                print(subregsector,reg['region'],reg['subregion'],urlp)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Education': '4664', 'Hospitality': '12085', 'Jobs Wanted': '17559', 'Marketing - PR': '6856', 'Other': '55318', 'Retail': '3906', 'Sales': '7532', 'Secretarial': '3871', 'Accounting': '3154', 'Architecture - Engineering': '2563', 'Art - Design': '1721', 'Business Development': '225', 'Construction': '2692', 'Consulting': '589', 'Executive': '2064', 'HR - Recruiting': '790', 'IT - Telecom': '2853', 'Medical - Health': '2506'}\n",
      "saqqara\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a3e0d2d8b394>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwrite_OLXJobUrls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msector\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdatast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m136\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-c988c96ec221>\u001b[0m in \u001b[0;36mwrite_OLXJobUrls\u001b[0;34m(sector, datast)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0msubregsector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhref\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msubreghref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[1;31m#print(href)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0murlpages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_OLXJobPageUrls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[1;31m#print(len(urlpages))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0murlp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0murlpages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-462d34da6d56>\u001b[0m in \u001b[0;36mget_OLXJobPageUrls\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mnewurl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/?page='\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewurl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[1;31m# post-process response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 544\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0;32m-> 1361\u001b[0;31m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0;32m-> 1318\u001b[0;31m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0m\u001b[1;32m   1319\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# timeout error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                 encode_chunked=False):\n\u001b[1;32m   1238\u001b[0m         \u001b[1;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1283\u001b[0m             \u001b[1;31m# default charset of iso-8859-1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'body'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m     def request(self, method, url, body=None, headers={}, *,\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb\"\\r\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msock\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mNotConnected\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1390\u001b[0m             \u001b[1;34m\"Connect to a host on a given (SSL) port.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tunnel_host\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[1;34m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         self.sock = self._create_connection(\n\u001b[0;32m--> 936\u001b[0;31m             (self.host,self.port), self.timeout, self.source_address)\n\u001b[0m\u001b[1;32m    937\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetsockopt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIPPROTO_TCP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTCP_NODELAY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m                 \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(sector)\n",
    "write_OLXJobUrls(sector,datast=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     download_date          region        fregname  totalposts  \\\n",
      "270       11302017      New Valley      new-valley         363   \n",
      "271       11302017      New Valley      new-valley         363   \n",
      "272       11302017      New Valley      new-valley         363   \n",
      "273       11302017      New Valley      new-valley         363   \n",
      "274       11302017      New Valley      new-valley         363   \n",
      "275       11302017      New Valley      new-valley         363   \n",
      "276       11302017       Beni Suef       beni-suef        7282   \n",
      "277       11302017       Beni Suef       beni-suef        7282   \n",
      "278       11302017       Beni Suef       beni-suef        7282   \n",
      "279       11302017       Beni Suef       beni-suef        7282   \n",
      "280       11302017       Beni Suef       beni-suef        7282   \n",
      "281       11302017       Beni Suef       beni-suef        7282   \n",
      "282       11302017       Beni Suef       beni-suef        7282   \n",
      "283       11302017       Beni Suef       beni-suef        7282   \n",
      "284       11302017       Port Said       port-said       20581   \n",
      "285       11302017       Port Said       port-said       20581   \n",
      "286       11302017       Port Said       port-said       20581   \n",
      "287       11302017       Port Said       port-said       20581   \n",
      "288       11302017       Port Said       port-said       20581   \n",
      "289       11302017       Port Said       port-said       20581   \n",
      "290       11302017       Port Said       port-said       20581   \n",
      "291       11302017     South Sinai     south-sinai        8571   \n",
      "292       11302017     South Sinai     south-sinai        8571   \n",
      "293       11302017     South Sinai     south-sinai        8571   \n",
      "294       11302017     South Sinai     south-sinai        8571   \n",
      "295       11302017     South Sinai     south-sinai        8571   \n",
      "296       11302017     South Sinai     south-sinai        8571   \n",
      "297       11302017     South Sinai     south-sinai        8571   \n",
      "298       11302017     South Sinai     south-sinai        8571   \n",
      "299       11302017     South Sinai     south-sinai        8571   \n",
      "..             ...             ...             ...         ...   \n",
      "330       11302017            Qena            qena        6939   \n",
      "331       11302017            Qena            qena        6939   \n",
      "332       11302017            Qena            qena        6939   \n",
      "333       11302017            Qena            qena        6939   \n",
      "334       11302017            Qena            qena        6939   \n",
      "335       11302017            Qena            qena        6939   \n",
      "336       11302017            Qena            qena        6939   \n",
      "337       11302017            Qena            qena        6939   \n",
      "338       11302017            Qena            qena        6939   \n",
      "339       11302017  Kafr al-Sheikh  kafr-al-sheikh        8457   \n",
      "340       11302017  Kafr al-Sheikh  kafr-al-sheikh        8457   \n",
      "341       11302017  Kafr al-Sheikh  kafr-al-sheikh        8457   \n",
      "342       11302017  Kafr al-Sheikh  kafr-al-sheikh        8457   \n",
      "343       11302017  Kafr al-Sheikh  kafr-al-sheikh        8457   \n",
      "344       11302017  Kafr al-Sheikh  kafr-al-sheikh        8457   \n",
      "345       11302017  Kafr al-Sheikh  kafr-al-sheikh        8457   \n",
      "346       11302017  Kafr al-Sheikh  kafr-al-sheikh        8457   \n",
      "347       11302017  Kafr al-Sheikh  kafr-al-sheikh        8457   \n",
      "348       11302017  Kafr al-Sheikh  kafr-al-sheikh        8457   \n",
      "349       11302017  Kafr al-Sheikh  kafr-al-sheikh        8457   \n",
      "350       11302017          Matruh          matruh       48462   \n",
      "351       11302017          Matruh          matruh       48462   \n",
      "352       11302017          Matruh          matruh       48462   \n",
      "353       11302017          Matruh          matruh       48462   \n",
      "354       11302017          Matruh          matruh       48462   \n",
      "355       11302017          Matruh          matruh       48462   \n",
      "356       11302017          Matruh          matruh       48462   \n",
      "357       11302017          Matruh          matruh       48462   \n",
      "358       11302017          Matruh          matruh       48462   \n",
      "359       11302017          Matruh          matruh       48462   \n",
      "\n",
      "               subregion          fsubregname  subposts  \n",
      "270                Balat                balat        10  \n",
      "271               Dakhla               dakhla        32  \n",
      "272              Farafra              farafra        47  \n",
      "273               Kharga               kharga       231  \n",
      "274                  Mut                  mut        42  \n",
      "275                Paris                paris         1  \n",
      "276             Al Feshn             al-feshn       236  \n",
      "277             Al Wasty             al-wasty       571  \n",
      "278                 Beba                 beba       192  \n",
      "279       Beni Suef City       beni-suef-city      3788  \n",
      "280              Ehnasia              ehnasia       137  \n",
      "281               Nasser               nasser       185  \n",
      "282        New Beni Suef        new-beni-suef      1987  \n",
      "283              Samasta              samasta       186  \n",
      "284        Arab District        arab-district       616  \n",
      "285      Dawahy District      dawahy-district      1964  \n",
      "286      Ganoub District      ganoub-district      1309  \n",
      "287      Manakh District      manakh-district       758  \n",
      "288           Port Fouad           port-fouad      6803  \n",
      "289       Sharq District       sharq-district      7568  \n",
      "290      Zohour District      zohour-district      1563  \n",
      "291           Abu Rudeis           abu-rudeis        21  \n",
      "292          Abu Zenimah          abu-zenimah        17  \n",
      "293                Dahab                dahab       102  \n",
      "294              Nuweiba              nuweiba        24  \n",
      "295             Ras Sedr             ras-sedr      2682  \n",
      "296             Ras Sidr             ras-sidr      2090  \n",
      "297      Sharm al-Sheikh      sharm-al-sheikh      3370  \n",
      "298        St. Catherine         st-catherine         4  \n",
      "299                 Taba                 taba         8  \n",
      "..                   ...                  ...       ...  \n",
      "330            Abu Tisht            abu-tisht       206  \n",
      "331             Farshout             farshout       106  \n",
      "332                 Keft                 keft       128  \n",
      "333          Nag Hammadi          nag-hammadi      1019  \n",
      "334               Nakada               nakada        92  \n",
      "335            Qena City            qena-city      4989  \n",
      "336                 Quos                 quos       266  \n",
      "337                 Shna                 shna        31  \n",
      "338                 Wakf                 wakf       102  \n",
      "339               Baltim               baltim       482  \n",
      "340                Bella                bella       378  \n",
      "341               Brolos               brolos        95  \n",
      "342               Desouk               desouk      1142  \n",
      "343                 Fouh                 fouh       140  \n",
      "344               Hamoul               hamoul       351  \n",
      "345  Kafr al-Sheikh City  kafr-al-sheikh-city      5118  \n",
      "346              Motobas              motobas       104  \n",
      "347               Qaleen               qaleen       243  \n",
      "348               Riyadh               riyadh       183  \n",
      "349           Sidi Salem           sidi-salem       221  \n",
      "350              Alamein              alamein      1334  \n",
      "351               Barany               barany         5  \n",
      "352                Dabaa                dabaa        39  \n",
      "353               Hammam               hammam       307  \n",
      "354    Marina El Alamein    marina-el-alamein       850  \n",
      "355        Marsa Matrouh        marsa-matrouh      6384  \n",
      "356               Nagela               nagela        85  \n",
      "357          North Coast          north-coast     39228  \n",
      "358              Salloum              salloum         4  \n",
      "359                 Siwa                 siwa       226  \n",
      "\n",
      "[90 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data[270:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getOLXJobPageUrls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-595e53837533>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://olx.com.eg/en/jobs-services/retail/alexandria/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0murlsall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetOLXJobPageUrls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murlsall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'getOLXJobPageUrls' is not defined"
     ]
    }
   ],
   "source": [
    "url = 'https://olx.com.eg/en/jobs-services/retail/alexandria/'\n",
    "urlsall = getOLXJobPageUrls(url)\n",
    "print(urlsall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#extract data from individual job ads \n",
    "import re\n",
    "import googletrans\n",
    "\n",
    "def request_until_succeed(url):\n",
    "    req = urlrequest.Request(url)\n",
    "    count = 1\n",
    "    while count <= 5:\n",
    "        try: \n",
    "            response = urlrequest.urlopen(req)\n",
    "            if response.getcode() == 200:\n",
    "                return(response)\n",
    "        except Exception:\n",
    "            print(\"Exception\")\n",
    "            time.sleep(1)\n",
    "            print(\"Error for URL %s: %s\" % (url, datetime.datetime.now()))\n",
    "        count+=1\n",
    "    return(None)\n",
    "\n",
    "def get_OLXjobdata(url,sector,region,subregion):\n",
    "    \n",
    "    fields = ['Experience Level','Employment Type','Education Level','Type','Compensation']\n",
    "    fielddata = {}\n",
    "    \n",
    "    #print(url)\n",
    "    response = request_until_succeed(url)\n",
    "    \n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    \n",
    "    tempurl = url.split('ad/')[1]\n",
    "    urlshort = tempurl.replace('.html','')\n",
    "    \n",
    "    #get content for ad posting data and check if available as some are no longer available\n",
    "    addata = soup.find('span',attrs={'class':'pdingleft10 brlefte5'})\n",
    "    \n",
    "    ### note want to add in the actual time download if we are to use the page views as proxy    \n",
    "    datetimenow = time.strftime('%Y-%m-%d %H:%M')\n",
    "    \n",
    "    if addata is not None:\n",
    "        #print(addata)\n",
    "        addata = addata.get_text().strip()\n",
    "        m = re.search(r\"at (\\d+:\\d+, \\d+ \\w+ \\d+), Ad ID: (\\d+)\",addata)\n",
    "        date = m.group(1)\n",
    "        adid = m.group(2)\n",
    "        dateval = datetime.datetime.strptime(date,'%H:%M, %d %B %Y')\n",
    "        adpostdate = dateval.strftime('%Y-%m-%d %H:%M') # best time format for spreadsheet programs\n",
    "        #print(adid,adpostdate)\n",
    "\n",
    "        #get main content related to job\n",
    "        name_box = soup.find_all('div', attrs={'class': \"clr descriptioncontent marginbott20\"})\n",
    "    \n",
    "        for name in name_box:\n",
    "            #print(name)\n",
    "            newnames = name.find_all('td', attrs={'class' : 'col'})\n",
    "            #print(newnames)\n",
    "            for name in newnames:\n",
    "                cat = name.find('th').get_text().strip()\n",
    "                catval = name.find('td').get_text().strip()\n",
    "                fielddata[cat] = catval\n",
    "                #print(cat)\n",
    "                #print(catval)\n",
    "\n",
    "        #note that not all categories are always included in a job advertisement so we have to make sure there are contingencies\n",
    "        for f in fields:\n",
    "            if f not in fielddata:\n",
    "                fielddata[f] = np.NAN\n",
    "        \n",
    "        content = soup.find('div', attrs={'class':\"clr\", 'id':'textContent'}).get_text().strip()\n",
    "        translator = googletrans.Translator()\n",
    "        try:\n",
    "            translated = translator.translate(content)\n",
    "            jobcontent = translated.text\n",
    "        except:\n",
    "            jobcontent = \"\"\n",
    "    \n",
    "        #format translated text \n",
    "        jobcontent = jobcontent.replace('\\r',' ').replace('\\n','>').encode('utf-8')\n",
    "        #print(content,jobcontent)\n",
    "\n",
    "        views = soup.find_all('div',attrs={'class':'pdingtop10'})\n",
    "        #print(views)\n",
    "        for v in views:\n",
    "            if 'Views' in str(v):\n",
    "                m = re.search(r\"Views:<strong>(\\d+)</strong>\", str(v))\n",
    "                num_views = m.group(1)\n",
    "                #print(num_views)\n",
    "\n",
    "        return(datetimenow, adid,urlshort,adpostdate,sector,region,subregion,jobcontent,num_views,fielddata['Experience Level'],fielddata['Employment Type'],\n",
    "                fielddata['Education Level'],fielddata['Type'],fielddata['Compensation'])\n",
    "\n",
    "    else:\n",
    "        return(datetimenow, np.NAN,urlshort,np.NAN,sector,region,subregion,np.NAN,np.NAN,np.NAN,np.NAN,np.NAN,np.NAN,np.NAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2017-12-07 19:18',\n",
       " '122611763',\n",
       " '-ID8isW5',\n",
       " '2017-12-03 17:08',\n",
       " nan,\n",
       " nan,\n",
       " nan,\n",
       " b'Production workers are required in a plastic factory>Salary 3000>There is accommodation for expatriates>Health and social insurance for employees and families>Upgrades and quick incentives>\\xc2\\xa0\\xd9\\x84\\xd9\\x84\\xd8\\xaa\\xd9\\x88\\xd8\\xa7\\xd8\\xb5\\xd9\\x84 \\xd9\\x81\\xd9\\x88\\xd8\\xb1\\xd8\\xb1\\xd8\\xb1\\xd8\\xb1\\xd8\\xb1\\xd8\\xb1\\xd8\\xa7 \\xd8\\xa7\\xd9\\x84\\xd8\\xa7\\xd8\\xb3\\xd8\\xaa\\xd8\\xa7\\xd8\\xb0\\xd9\\x87 \\xd8\\xa7\\xd9\\x86\\xd8\\xac\\xd9\\x8a olol 634 - show phone -',\n",
       " '592',\n",
       " 'Entry level',\n",
       " 'Full-time',\n",
       " 'Diploma',\n",
       " 'Employer',\n",
       " '3,000')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_OLXjobdata('https://olx.com.eg/en/ad/-ID8isW5.html',np.NAN,np.NAN,np.NAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dateval = '12012017'\n",
    "\n",
    "def write_OLXjobpagedata(datast=0):\n",
    "    \n",
    "    #sector names\n",
    "    sectornames = []\n",
    "    for key, val in sector.items():\n",
    "        sectornames.append(key)\n",
    "    \n",
    "    urllist = {}\n",
    "    \n",
    "    op = 'a'\n",
    "    if datast == 0:\n",
    "        op = 'w'\n",
    "    \n",
    "    #write out the data for each job page\n",
    "    with open('OLX_jobpagedata_'+dateval+'.csv', op, newline='') as file:\n",
    "        w = csv.writer(file)\n",
    "        if datast == 0:\n",
    "            w.writerow([\"downloaddate\",\"id\",\"url\",\"postdate\",\"sector\",\"region\",\"subregion\",\"jobcontent\",\"num_views\",\"explevel\",\"emptype\",\"educlevel\",\"employertype\",\"compensation\"])\n",
    "    \n",
    "        #loop through 365 qism areas to call the files containing the different urls\n",
    "        for i, reg in data[datast:].iterrows():\n",
    "            fsubregname = reg['fsubregname']\n",
    "            print(fsubregname)\n",
    "            urllist[fsubregname]=pd.read_csv('OLX_joburls_'+reg['fsubregname']+'_'+dateval+'.csv')\n",
    "            for j, row in urllist[fsubregname].iterrows():\n",
    "                temp = row['jobpageurl'].split('ad/')[1]\n",
    "                newurl = 'https://olx.com.eg/en/ad/'+temp\n",
    "                rowdata = get_OLXjobdata(newurl,row['sector'],row['region'],row['subregion'])\n",
    "                w.writerow(rowdata)\n",
    "                time.sleep(1)   \n",
    "  \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fouh\n",
      "hamoul\n",
      "kafr-al-sheikh-city\n",
      "motobas\n",
      "qaleen\n",
      "riyadh\n",
      "sidi-salem\n",
      "alamein\n",
      "barany\n",
      "dabaa\n",
      "hammam\n",
      "marina-el-alamein\n",
      "marsa-matrouh\n",
      "nagela\n",
      "north-coast\n",
      "salloum\n",
      "siwa\n"
     ]
    }
   ],
   "source": [
    "#try to resurvey giza-district (114, 127).  Last download sharq district.\n",
    "write_OLXjobpagedata(datast=343)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://olx.com.eg/en/ad/1600-ID8nedy.html\n",
      "123746888 2017-11-26 21:20\n",
      "مطعم بشارع فؤاد - محطة الرمل - الاسكندرية\n",
      "اجازة يوم اسبوعيا \n",
      "العمل 9 ساعات \n",
      "السن اقل من 40 عام\n",
      "يشترط التفرغ\n",
      "ارسل رسالة تحتوي على البيانات وسوف يتم الاتصال لتحديد ميعاد المقابلة Fouad Street Restaurant - Raml Station - Alexandria >Weekday vacation >Working 9 hours >Age less than 40 years >Full time required >Send a message containing the data and will be contacted to determine the appointment\n",
      "2520\n",
      "('123746888', '2017-11-26 21:20', nan, nan, nan, 'Fouad Street Restaurant - Raml Station - Alexandria >Weekday vacation >Working 9 hours >Age less than 40 years >Full time required >Send a message containing the data and will be contacted to determine the appointment', '2520', 'Entry level', 'Full-time', 'Diploma', 'Employer', '1,600')\n"
     ]
    }
   ],
   "source": [
    "# test the above function on one of the urls\n",
    "url = 'https://olx.com.eg/en/ad/1600-ID8nedy.html'\n",
    "region = np.NAN\n",
    "subregion = np.NAN\n",
    "sector = np.NAN\n",
    "print(get_OLXjobdata(url,sector,region,subregion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan {'Education': 'https://olx.com.eg/en/jobs-services/education/', 'Hospitality': 'https://olx.com.eg/en/jobs-services/hospitality/', 'Jobs Wanted': 'https://olx.com.eg/en/jobs-services/jobs-wanted/', 'Marketing - PR': 'https://olx.com.eg/en/jobs-services/marketingpr/', 'Other': 'https://olx.com.eg/en/jobs-services/jobs-other/', 'Retail': 'https://olx.com.eg/en/jobs-services/retail/', 'Sales': 'https://olx.com.eg/en/jobs-services/sales/', 'Secretarial': 'https://olx.com.eg/en/jobs-services/secretarial/', 'Accounting': 'https://olx.com.eg/en/jobs-services/accounting/', 'Architecture - Engineering': 'https://olx.com.eg/en/jobs-services/architectureengineering/', 'Art - Design': 'https://olx.com.eg/en/jobs-services/artdesign/', 'Business Development': 'https://olx.com.eg/en/jobs-services/business-development/', 'Construction': 'https://olx.com.eg/en/jobs-services/construction/', 'Consulting': 'https://olx.com.eg/en/jobs-services/consulting/', 'Executive': 'https://olx.com.eg/en/jobs-services/executive/', 'HR - Recruiting': 'https://olx.com.eg/en/jobs-services/hrrecruiting/', 'IT - Telecom': 'https://olx.com.eg/en/jobs-services/ittelecom/', 'Medical - Health': 'https://olx.com.eg/en/jobs-services/medicalhealth/'}\n"
     ]
    }
   ],
   "source": [
    "print(sector, href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
